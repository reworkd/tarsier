------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[ $ 1 ] [ @ 2 ]    **OpenAI [ 3 ] Documentation**            [ @ 4 ] API reference                                                    [ $ 5 ] Sign Up [ $ 6 ] Log In [ $ 7 ] Q

  [ @ 8 ] Topics
                                        [ 30 ] Welcome to the OpenAl Developer Forum!                                                                      [ $ 29 ] ×
  [ $ 9 ] More
                                       [ 31 ] What to know before posting a new question:
  [ $ 10 ] RESOURCES
                                       1. [ 32 ] Search the forum for similar topics - the question might have been discussed before.
  [ @ 11 ] Documentation               2. [ 33 ] If the question relates account issues ( e.g., billing and login issues ), please contact us through our [ @ 35 ] Help Center [ 34 ]
  [ @ 12 ] API reference               3. [ 36 ] Please be kind and helpful in conversations!
  [ @ 13 ] Help center
  [ $ 14 ] CATEGORIES                [ @ 37 ] How to address a GPT in its instructions:
                                     [ @ 38 ] Prompting [ @ 39 ] chatgpt, [ @ 40 ] gpts, [ @ 41 ] assistants
  [ @ 15 ] Announcements
  [ @ 16 ] API                                                                                                                                     [ @ 42 ] Nov 19
                                      [ @ 46 ] [ 47 ] luona.dev                                                                 [ @ 48 ] 8d
  [ @ 17 ] Plugins / Actions Dev                                                                                                                      [ 43 ] 1/4
                                             [ 49 ] | conducted an experiment to figure out how to best address GPTs in their instructions. I gave it 5 [ 44 ] Nov
  [ @ 18 ] Prompting
                                             different instruction about the meaning of a made up word in a fantasy language and then asked it
  [ @ 19 ] Documentation                     about it. I addressed it with
  [ @ 20 ] All categories                    [ 50 ]" You"," 1"," The assistant"," The GPT" and"{ name of the GPT}". I tested each of the 120
                                             permutations 6 times. This is what I found out:
  [ $ 21 ] TAGS                                 •
                                                  [ 52 ] You [ 51 ] is by far the strongest way of addressing a GPT. In over 50 % of the cases, the GPT
  [ @ 22 ] chatgpt                                answered with the knowledge associated to" you".
  [ @ 23 ] gpt - 4                             ⚫ [ 54 ] The last instruction [ 53 ] is by far the most convincing instruction, with the last info being
                                                  returned 50 % of the time.
  [ @ 24 ] api
                                             [ 55 ] Here you can find a [ @ 57 ] detailed write - up of the experiment 74 [ 56 ].
  [ @ 25 ] plugin - development
  [ @ 26 ] lost - user
  [ 27 ] = All tags                                                                                                [ $ 58 ] 11  [ $ 59 ]           [ @ 45 ] 7d

                                             [ @ 60 ] Custom GPT Instructions: using 2nd vs. 3rd person

                                              [ 62 ] created     [ @ 65 ]            [ 68 ] 3   [ 70 ] 449   [ 72 ] 4
                                                                 last reply         [ 69 ] replies [ 71 ] views [ 73 ] users
                                              [ @ 63 ]  [ 64 ] 8d
                                                                 [ @ 66 ]  [ 67 ] 7d
                                                                                                                               [ $ 61 ] ▼
                                              [ 74 ] 12  [ 76 ] 1 [ @ 78 ] L [ @ 79 ]    [ @ 80 ] V
                                               [ 75 ] likes [ 77 ] link

                                      [ @ 81 ] [ @ 82 ] anderskkehlet                                                           [ @ 83 ] 8d
                                             [ 84 ] Nice test. It would not have occurred to me to use contradicting info.

                                                                                                                     [ $ 85 ]   [ $ 86 ]

                                      [ @ 87 ] [ 88 ] LinqLover                                                                 [ @ 89 ] 8d
                                            [ 90 ] Very nice! We need for evidence on prompt tuning ( at least there has been posted very few
                                             research on this in this forum ).

                                                                                                                     [ $ 91 ]   [ $ 92 ]

                                      [ @ 93 ] [ @ 94 ] vspritola                                                    [ $ 95 ] 1 [ @ 96 ] 7d
                                       V [ 97 ] This is a fascinating experiment! Interestingly this is also true of humans. We also recall best ( and
                                             therefore can act on ) information at the end and the beginning of a text passage. My take on this is that
                                             it may be a feature of the training data where it is simulating how people are writing ( i.e. processing )
                                             based on text that came before.
                                             [ 98 ] Also if you think how you yourself would give or receive instructions best. You and your name
                                             would be the most impactful. Granted here there may be some underlying schemas about the name
                                             that are not part of the training, but regardless this also aligns with human tendencies.
                                             [ 99 ] EDIT: Now after looking deeper into the article, it seems that position 3/5 was still more impactful
                                             than the beginning. This does not fit as well to the picture, but I see I may have thought about this
                                             wrong. The experiment doesn't measure just simple recall, where recency and primacy effects should
                                             dominate, but also has a component of solving contradiction. In many cases the last" revision" could be
                                             the correct one in texts. But this is now pure speculation. Very interesting.

                                                                                                                 [ $ 100 ] 1 ♥ [ $ 101 ]

                                                                                                                         [ $ 102 ] Reply
                                      [ 103 ] Related Topics
                                       [ 104 ] Topic                                                                        [ 105 ] Replies [ 106 ] Views [ 107 ] Activity
                                       [ @ 108 ] Providing context to the Chat API before a conversation
                                       [ @ 109 ] Prompting                                                                    [ $ 115 ] 7 [ 116 ] 10.0k [ @ 117 ] May 8
                                       [ @ 110 ] gpt - 4, [ @ 111 ] gpt - 35 - turbo, [ @ 112 ] chatml, [ @ 113 ] chatml - system, [ @ 114 ] chatml - user

                                       [ @ 118 ] Custom GPT Instructions: using 2nd vs. 3rd person                           [ $ 124 ] 18 [ 125 ] 1.0k [ @ 126 ] 14h
                                       [ @ 119 ] Prompting [ @ 120 ] chatgpt, [ @ 121 ] custom - gpt, [ @ 122 ] custom - instructions, [ @ 123 ] tp - 1

                                       [ @ 127 ] How to provide context so gpt - 3 continues the conversation?                [ $ 129 ] 1  130 ] 218             23
                                               API                                                                                        [           [ @ 131 ] Sep
                                       [ @ 128 ]

                                       [ @ 132 ] GPT - 4 keeps lying instead of saying" I don't know"!                        [ $ 136 ] 5 [ 137 ] 2.7k [ @ 138 ] Jun 21
                                       [ @ 133 ] Prompting [ @ 134 ] gpt - 4, [ @ 135 ] hallucinations

                                       [ @ 139 ] Wow in a few lines
                                                                                                                              [ $ 141 ] 3 [ 142 ] 1.1k [ @ 143 ] Jul '22
                                       [ @ 140 ] API








  [ $ 28 ]

 [ 144 ] OpenAl © 2015-                 [ 146 ] Research    [ 149 ] Product            [ 154 ] Safety      [ 157 ] Help       [ 159 ] Developers       [ 163 ] Company
 2023                                   [ @ 147 ] Overview  [ @ 150 ] Overview         [ @ 155 ] Overview  [ @ 158 ] Support  [ @ 160 ] Documentation  [ @ 164 ] About
                                        [ @ 148 ] Index     [ @ 151 ] Customer stories [ @ 156 ] Security                     [ @ 161 ] Service status [ @ 165 ] Blog
 [ 145 ] The OpenAl developer forum
 is a place to connect with other                           [ @ 152 ] Safety standards                                        [ @ 162 ] Examples       [ @ 166 ] Careers
 people building with OpenAl models.                        [ @ 153 ] Pricing                                                                          [ @ 167 ] Charter

 [ @ 168 ] Terms & policies [ @ 169 ] Privacy policy [ @ 170 ] ] Brand guidelines                                      [ @ 171 ]    [ @ 172 ] ► [ @ 173 ]    [ @ 174 ] in
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Token count: 1941